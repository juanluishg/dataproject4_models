{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Context_Model.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "computational-auckland"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from os import listdir\n",
        "from numpy import zeros\n",
        "from numpy import asarray\n",
        "from numpy import savez_compressed\n",
        "from pandas import read_csv\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array"
      ],
      "id": "computational-auckland",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38Uq6c9T_vaR",
        "outputId": "60aeb6fd-3405-4570-dfa7-87002570da38"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "38Uq6c9T_vaR",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "correct-width",
        "outputId": "212292e1-497a-4f16-8494-3d17d9cabeab"
      },
      "source": [
        "df = pd.read_csv('emotics_annotations_clean_v4.csv')\n",
        "df.head()"
      ],
      "id": "correct-width",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d29f08c61f64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets/emotics_annotations_clean_v4.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/emotics_annotations_clean_v4.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skilled-departure"
      },
      "source": [
        "# create a mapping of tags to integers given the loaded mapping file\n",
        "def create_tag_mapping(mapping_csv):\n",
        "\t# create a set of all known tags\n",
        "\tlabels = set()\n",
        "\tfor i in range(len(mapping_csv)):\n",
        "\t\t# convert spaced separated tags into an array of tags\n",
        "\t\ttags = mapping_csv['categories'][i].split(',')\n",
        "\t\t# add tags to the set of known labels\n",
        "\t\tlabels.update(tags)\n",
        "\t# convert set of labels to a list to list\n",
        "\tlabels = list(labels)\n",
        "\t# order set alphabetically\n",
        "\tlabels.sort()\n",
        "\t# dict that maps labels to integers, and the reverse\n",
        "\tlabels_map = {labels[i]:i for i in range(len(labels))}\n",
        "\tinv_labels_map = {i:labels[i] for i in range(len(labels))}\n",
        "\treturn labels_map, inv_labels_map"
      ],
      "id": "skilled-departure",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "similar-angel"
      },
      "source": [
        "mapping, inv_mapping = create_tag_mapping(df)\n",
        "print(len(mapping))\n",
        "print(mapping)"
      ],
      "id": "similar-angel",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quantitative-metropolitan"
      },
      "source": [
        "# create a mapping of filename to tags\n",
        "def create_file_mapping(mapping_csv):\n",
        "\tmapping = dict()\n",
        "\tfor i in range(len(mapping_csv)):\n",
        "\t\tname, tags = mapping_csv['filename'][i], mapping_csv['categories'][i]\n",
        "\t\tmapping[name] = tags.split(',')\n",
        "\treturn mapping"
      ],
      "id": "quantitative-metropolitan",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "popular-binding"
      },
      "source": [
        "# create a one hot encoding for one list of tags\n",
        "def one_hot_encode(tags, mapping):\n",
        "\t# create empty vector\n",
        "\tencoding = zeros(len(mapping), dtype='uint8')\n",
        "\t# mark 1 for each tag in the vector\n",
        "\tfor tag in tags:\n",
        "\t\tencoding[mapping[tag]] = 1\n",
        "\treturn encoding"
      ],
      "id": "popular-binding",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "every-guyana"
      },
      "source": [
        "# load all images into memory\n",
        "def load_dataset(path, file_mapping, tag_mapping):\n",
        "    photos, targets = list(), list()\n",
        "    # enumerate files in the directory\n",
        "    for f in listdir(path):\n",
        "        f = path + f + '/images/'\n",
        "        for filename in listdir(f):\n",
        "            # load image\n",
        "            photo = load_img(f + filename, target_size=(128,128))\n",
        "            # convert to numpy array\n",
        "            photo = img_to_array(photo, dtype='uint8')\n",
        "            # get tags\n",
        "            try:\n",
        "                tags = file_mapping[filename]\n",
        "            except:\n",
        "                print(filename)\n",
        "            # one hot encode tags\n",
        "            target = one_hot_encode(tags, tag_mapping)\n",
        "            # store\n",
        "            photos.append(photo)\n",
        "            targets.append(target)\n",
        "    X = asarray(photos, dtype='uint8')\n",
        "    y = asarray(targets, dtype='uint8')\n",
        "    return X, y"
      ],
      "id": "every-guyana",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "enormous-suspect"
      },
      "source": [
        "# load the mapping file\n",
        "filename = 'emotics_annotations_clean_v4.csv'\n",
        "mapping_csv = pd.read_csv(filename)\n",
        "# create a mapping of tags to integers\n",
        "tag_mapping, _ = create_tag_mapping(mapping_csv)\n",
        "# create a mapping of filenames to tag lists\n",
        "file_mapping = create_file_mapping(mapping_csv)\n",
        "# load the jpeg images\n",
        "folder = 'raw_data/EMOTIC_DATASET/emotic/'\n",
        "X, y = load_dataset(folder, file_mapping, tag_mapping)\n",
        "print(X.shape, y.shape)\n",
        "# save both arrays to one file in compressed format\n",
        "savez_compressed('planet_data.npz', X, y)"
      ],
      "id": "enormous-suspect",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "understood-consultancy",
        "outputId": "62a9d2ad-8107-41ae-dbe7-702310c1583f"
      },
      "source": [
        "# load prepared planet dataset\n",
        "from numpy import load\n",
        "data = load('drive/MyDrive/planet_data.npz')\n",
        "X, y = data['arr_0'], data['arr_1']\n",
        "print('Loaded: ', X.shape, y.shape)"
      ],
      "id": "understood-consultancy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded:  (7749, 128, 128, 3) (7749, 26)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scientific-irrigation"
      },
      "source": [
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\tdata = load('drive/MyDrive/planet_data.npz')\n",
        "\tX, y = data['arr_0'], data['arr_1']\n",
        "\t# separate into train and test datasets\n",
        "\ttrainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\tprint(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
        "\treturn trainX, trainY, testX, testY"
      ],
      "id": "scientific-irrigation",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqiKAEqJAy1_"
      },
      "source": [
        "import tensorflow as tf\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
      ],
      "id": "oqiKAEqJAy1_",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "occasional-cassette",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe726da0-ef5e-48f4-ab8f-8aea55c19ed4"
      },
      "source": [
        "# test f-beta score\n",
        "from numpy import load\n",
        "from numpy import ones\n",
        "from numpy import asarray\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\tdata = load('drive/MyDrive/planet_data.npz')\n",
        "\tX, y = data['arr_0'], data['arr_1']\n",
        "\t# separate into train and test datasets\n",
        "\ttrainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\tprint(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
        "\treturn trainX, trainY, testX, testY\n",
        "\n",
        "# load dataset\n",
        "trainX, trainY, testX, testY = load_dataset()\n",
        "# make all one predictions\n",
        "train_yhat = asarray([ones(trainY.shape[1]) for _ in range(trainY.shape[0])])\n",
        "test_yhat = asarray([ones(testY.shape[1]) for _ in range(testY.shape[0])])\n",
        "# evaluate predictions\n",
        "train_score = fbeta_score(trainY, train_yhat, 2, average='samples')\n",
        "test_score = fbeta_score(testY, test_yhat, 2, average='samples')\n",
        "print('All Ones: train=%.3f, test=%.3f' % (train_score, test_score))"
      ],
      "id": "occasional-cassette",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5424, 128, 128, 3) (5424, 26) (2325, 128, 128, 3) (2325, 26)\n",
            "All Ones: train=0.281, test=0.280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "satellite-terrorist"
      },
      "source": [
        "from keras import backend\n",
        "\n",
        "# calculate fbeta score for multi-class/label classification\n",
        "def fbeta(y_true, y_pred, beta=2):\n",
        "\t# clip predictions\n",
        "\ty_pred = backend.clip(y_pred, 0, 1)\n",
        "\t# calculate elements\n",
        "\ttp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
        "\tfp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
        "\tfn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
        "\t# calculate precision\n",
        "\tp = tp / (tp + fp + backend.epsilon())\n",
        "\t# calculate recall\n",
        "\tr = tp / (tp + fn + backend.epsilon())\n",
        "\t# calculate fbeta, averaged across each class\n",
        "\tbb = beta ** 2\n",
        "\tfbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
        "\treturn fbeta_score"
      ],
      "id": "satellite-terrorist",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "comparative-appearance",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca431f07-1f4a-4046-d006-c6eb223d857a"
      },
      "source": [
        "# load dataset\n",
        "trainX, trainY, testX, testY = load_dataset()\n",
        "# make all one predictions\n",
        "train_yhat = asarray([ones(trainY.shape[1]) for _ in range(trainY.shape[0])])\n",
        "test_yhat = asarray([ones(testY.shape[1]) for _ in range(testY.shape[0])])\n",
        "# evaluate predictions with sklearn\n",
        "train_score = fbeta_score(trainY, train_yhat, 2, average='samples')\n",
        "test_score = fbeta_score(testY, test_yhat, 2, average='samples')\n",
        "print('All Ones (sklearn): train=%.3f, test=%.3f' % (train_score, test_score))\n",
        "# evaluate predictions with keras\n",
        "train_score = fbeta(backend.variable(trainY), backend.variable(train_yhat))\n",
        "test_score = fbeta(backend.variable(testY), backend.variable(test_yhat))\n",
        "print('All Ones (keras): train=%.3f, test=%.3f' % (train_score, test_score))"
      ],
      "id": "comparative-appearance",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5424, 128, 128, 3) (5424, 26) (2325, 128, 128, 3) (2325, 26)\n",
            "All Ones (sklearn): train=0.281, test=0.280\n",
            "All Ones (keras): train=0.281, test=0.280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loved-lodge"
      },
      "source": [
        "# baseline model for the planet dataset\n",
        "import sys\n",
        "from numpy import load\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.optimizers import SGD"
      ],
      "id": "loved-lodge",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "secondary-place"
      },
      "source": [
        "# define cnn model\n",
        "def define_model(in_shape=(128, 128, 3), out_shape=26):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=in_shape))\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "  model.add(Dense(out_shape, activation='sigmoid'))\n",
        "  # compile model\n",
        "  opt = SGD(lr=0.01, momentum=0.9)\n",
        "  model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=[fbeta])\n",
        "  return model"
      ],
      "id": "secondary-place",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arctic-graphic"
      },
      "source": [
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Cross Entropy Loss')\n",
        "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Fbeta')\n",
        "\tpyplot.plot(history.history['fbeta'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_fbeta'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "\tfilename = sys.argv[0].split('/')[-1]\n",
        "\tpyplot.savefig(filename + '_plot.png')\n",
        "\tpyplot.close()"
      ],
      "id": "arctic-graphic",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "identified-pendant"
      },
      "source": [
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "\t# load dataset\n",
        "\ttrainX, trainY, testX, testY = load_dataset()\n",
        "\t# create data generator\n",
        "\tdatagen = ImageDataGenerator(rescale=1.0/255.0,\n",
        "                              featurewise_center=True,\n",
        "                              featurewise_std_normalization=True,\n",
        "                              rotation_range=20,\n",
        "                              width_shift_range=0.2,\n",
        "                              height_shift_range=0.2,\n",
        "                              horizontal_flip=True,\n",
        "                              validation_split=0.2)\n",
        "\t# prepare iterators\n",
        "\ttrain_it = datagen.flow(trainX, trainY, batch_size=128)\n",
        "\ttest_it = datagen.flow(testX, testY, batch_size=128)\n",
        "\t# define model\n",
        "\tmodel = define_model()\n",
        "\t# fit model\n",
        "\thistory = model.fit(train_it, steps_per_epoch=len(train_it)/2,\n",
        "\t\tvalidation_data=test_it, validation_steps=len(test_it)/2, epochs=50, batch_size=64)\n",
        "\t# evaluate model\n",
        "\tloss, fbeta = model.evaluate(test_it, steps=len(test_it), verbose=0)\n",
        "\tprint('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n",
        "\t# learning curves\n",
        "\tsummarize_diagnostics(history)"
      ],
      "id": "identified-pendant",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sapphire-illness",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "875e28ca-eca7-4de6-95f1-c14c90c6f0d8"
      },
      "source": [
        "run_test_harness()"
      ],
      "id": "sapphire-illness",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5424, 128, 128, 3) (5424, 26) (2325, 128, 128, 3) (2325, 26)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "21/21 [==============================] - 63s 727ms/step - loss: 0.5874 - fbeta: 0.2810 - val_loss: 0.2270 - val_fbeta: 0.3501\n",
            "Epoch 2/50\n",
            "21/21 [==============================] - 14s 639ms/step - loss: 0.2290 - fbeta: 0.2453 - val_loss: 0.2318 - val_fbeta: 0.1954\n",
            "Epoch 3/50\n",
            "21/21 [==============================] - 13s 629ms/step - loss: 0.2223 - fbeta: 0.3417 - val_loss: 0.2226 - val_fbeta: 0.3936\n",
            "Epoch 4/50\n",
            "21/21 [==============================] - 14s 636ms/step - loss: 0.2175 - fbeta: 0.3926 - val_loss: 0.2232 - val_fbeta: 0.3819\n",
            "Epoch 5/50\n",
            "22/21 [==============================] - ETA: 0s - loss: 0.2189 - fbeta: 0.3810"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Cqf7wbVXYRh"
      },
      "source": [
        ""
      ],
      "id": "3Cqf7wbVXYRh",
      "execution_count": null,
      "outputs": []
    }
  ]
}