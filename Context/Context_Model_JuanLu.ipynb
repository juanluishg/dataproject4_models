{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Context_Model.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "computational-auckland"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from os import listdir\n",
        "from numpy import zeros\n",
        "from numpy import asarray\n",
        "from numpy import savez_compressed\n",
        "from pandas import read_csv\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array"
      ],
      "id": "computational-auckland",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38Uq6c9T_vaR",
        "outputId": "3083535c-42c7-4a3a-c923-121a6e941429"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "38Uq6c9T_vaR",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "correct-width",
        "outputId": "212292e1-497a-4f16-8494-3d17d9cabeab"
      },
      "source": [
        "df = pd.read_csv('emotics_annotations_clean_v4.csv')\n",
        "df.head()"
      ],
      "id": "correct-width",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d29f08c61f64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets/emotics_annotations_clean_v4.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/emotics_annotations_clean_v4.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skilled-departure"
      },
      "source": [
        "# create a mapping of tags to integers given the loaded mapping file\n",
        "def create_tag_mapping(mapping_csv):\n",
        "\t# create a set of all known tags\n",
        "\tlabels = set()\n",
        "\tfor i in range(len(mapping_csv)):\n",
        "\t\t# convert spaced separated tags into an array of tags\n",
        "\t\ttags = mapping_csv['categories'][i].split(',')\n",
        "\t\t# add tags to the set of known labels\n",
        "\t\tlabels.update(tags)\n",
        "\t# convert set of labels to a list to list\n",
        "\tlabels = list(labels)\n",
        "\t# order set alphabetically\n",
        "\tlabels.sort()\n",
        "\t# dict that maps labels to integers, and the reverse\n",
        "\tlabels_map = {labels[i]:i for i in range(len(labels))}\n",
        "\tinv_labels_map = {i:labels[i] for i in range(len(labels))}\n",
        "\treturn labels_map, inv_labels_map"
      ],
      "id": "skilled-departure",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "similar-angel"
      },
      "source": [
        "mapping, inv_mapping = create_tag_mapping(df)\n",
        "print(len(mapping))\n",
        "print(mapping)"
      ],
      "id": "similar-angel",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quantitative-metropolitan"
      },
      "source": [
        "# create a mapping of filename to tags\n",
        "def create_file_mapping(mapping_csv):\n",
        "\tmapping = dict()\n",
        "\tfor i in range(len(mapping_csv)):\n",
        "\t\tname, tags = mapping_csv['filename'][i], mapping_csv['categories'][i]\n",
        "\t\tmapping[name] = tags.split(',')\n",
        "\treturn mapping"
      ],
      "id": "quantitative-metropolitan",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "popular-binding"
      },
      "source": [
        "# create a one hot encoding for one list of tags\n",
        "def one_hot_encode(tags, mapping):\n",
        "\t# create empty vector\n",
        "\tencoding = zeros(len(mapping), dtype='uint8')\n",
        "\t# mark 1 for each tag in the vector\n",
        "\tfor tag in tags:\n",
        "\t\tencoding[mapping[tag]] = 1\n",
        "\treturn encoding"
      ],
      "id": "popular-binding",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "every-guyana"
      },
      "source": [
        "# load all images into memory\n",
        "def load_dataset(path, file_mapping, tag_mapping):\n",
        "    photos, targets = list(), list()\n",
        "    # enumerate files in the directory\n",
        "    for f in listdir(path):\n",
        "        f = path + f + '/images/'\n",
        "        for filename in listdir(f):\n",
        "            # load image\n",
        "            photo = load_img(f + filename, target_size=(128,128))\n",
        "            # convert to numpy array\n",
        "            photo = img_to_array(photo, dtype='uint8')\n",
        "            # get tags\n",
        "            try:\n",
        "                tags = file_mapping[filename]\n",
        "            except:\n",
        "                print(filename)\n",
        "            # one hot encode tags\n",
        "            target = one_hot_encode(tags, tag_mapping)\n",
        "            # store\n",
        "            photos.append(photo)\n",
        "            targets.append(target)\n",
        "    X = asarray(photos, dtype='uint8')\n",
        "    y = asarray(targets, dtype='uint8')\n",
        "    return X, y"
      ],
      "id": "every-guyana",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "enormous-suspect"
      },
      "source": [
        "# load the mapping file\n",
        "filename = 'emotics_annotations_clean_v4.csv'\n",
        "mapping_csv = pd.read_csv(filename)\n",
        "# create a mapping of tags to integers\n",
        "tag_mapping, _ = create_tag_mapping(mapping_csv)\n",
        "# create a mapping of filenames to tag lists\n",
        "file_mapping = create_file_mapping(mapping_csv)\n",
        "# load the jpeg images\n",
        "folder = 'raw_data/EMOTIC_DATASET/emotic/'\n",
        "X, y = load_dataset(folder, file_mapping, tag_mapping)\n",
        "print(X.shape, y.shape)\n",
        "# save both arrays to one file in compressed format\n",
        "savez_compressed('planet_data.npz', X, y)"
      ],
      "id": "enormous-suspect",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "understood-consultancy",
        "outputId": "62a9d2ad-8107-41ae-dbe7-702310c1583f"
      },
      "source": [
        "# load prepared planet dataset\n",
        "from numpy import load\n",
        "data = load('drive/MyDrive/planet_data.npz')\n",
        "X, y = data['arr_0'], data['arr_1']\n",
        "print('Loaded: ', X.shape, y.shape)"
      ],
      "id": "understood-consultancy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded:  (7749, 128, 128, 3) (7749, 26)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scientific-irrigation"
      },
      "source": [
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\tdata = load('drive/MyDrive/planet_data.npz')\n",
        "\tX, y = data['arr_0'], data['arr_1']\n",
        "\t# separate into train and test datasets\n",
        "\ttrainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\tprint(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
        "\treturn trainX, trainY, testX, testY"
      ],
      "id": "scientific-irrigation",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqiKAEqJAy1_"
      },
      "source": [
        "import tensorflow as tf\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
      ],
      "id": "oqiKAEqJAy1_",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "occasional-cassette",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac775b02-5882-46f5-90c3-62b00b4dea81"
      },
      "source": [
        "# test f-beta score\n",
        "from numpy import load\n",
        "from numpy import ones\n",
        "from numpy import asarray\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\tdata = load('drive/MyDrive/planet_data.npz')\n",
        "\tX, y = data['arr_0'], data['arr_1']\n",
        "\t# separate into train and test datasets\n",
        "\ttrainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\tprint(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
        "\treturn trainX, trainY, testX, testY\n",
        "\n",
        "# load dataset\n",
        "trainX, trainY, testX, testY = load_dataset()\n",
        "# make all one predictions\n",
        "train_yhat = asarray([ones(trainY.shape[1]) for _ in range(trainY.shape[0])])\n",
        "test_yhat = asarray([ones(testY.shape[1]) for _ in range(testY.shape[0])])\n",
        "# evaluate predictions\n",
        "train_score = fbeta_score(trainY, train_yhat, 2, average='samples')\n",
        "test_score = fbeta_score(testY, test_yhat, 2, average='samples')\n",
        "print('All Ones: train=%.3f, test=%.3f' % (train_score, test_score))"
      ],
      "id": "occasional-cassette",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5424, 128, 128, 3) (5424, 26) (2325, 128, 128, 3) (2325, 26)\n",
            "All Ones: train=0.281, test=0.280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "satellite-terrorist"
      },
      "source": [
        "from keras import backend\n",
        "\n",
        "# calculate fbeta score for multi-class/label classification\n",
        "def fbeta(y_true, y_pred, beta=2):\n",
        "\t# clip predictions\n",
        "\ty_pred = backend.clip(y_pred, 0, 1)\n",
        "\t# calculate elements\n",
        "\ttp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
        "\tfp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
        "\tfn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
        "\t# calculate precision\n",
        "\tp = tp / (tp + fp + backend.epsilon())\n",
        "\t# calculate recall\n",
        "\tr = tp / (tp + fn + backend.epsilon())\n",
        "\t# calculate fbeta, averaged across each class\n",
        "\tbb = beta ** 2\n",
        "\tfbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
        "\treturn fbeta_score"
      ],
      "id": "satellite-terrorist",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "comparative-appearance",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1292283c-fe31-488c-e99f-eda6ad9affcc"
      },
      "source": [
        "# load dataset\n",
        "trainX, trainY, testX, testY = load_dataset()\n",
        "# make all one predictions\n",
        "train_yhat = asarray([ones(trainY.shape[1]) for _ in range(trainY.shape[0])])\n",
        "test_yhat = asarray([ones(testY.shape[1]) for _ in range(testY.shape[0])])\n",
        "# evaluate predictions with sklearn\n",
        "train_score = fbeta_score(trainY, train_yhat, 2, average='samples')\n",
        "test_score = fbeta_score(testY, test_yhat, 2, average='samples')\n",
        "print('All Ones (sklearn): train=%.3f, test=%.3f' % (train_score, test_score))\n",
        "# evaluate predictions with keras\n",
        "train_score = fbeta(backend.variable(trainY), backend.variable(train_yhat))\n",
        "test_score = fbeta(backend.variable(testY), backend.variable(test_yhat))\n",
        "print('All Ones (keras): train=%.3f, test=%.3f' % (train_score, test_score))"
      ],
      "id": "comparative-appearance",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5424, 128, 128, 3) (5424, 26) (2325, 128, 128, 3) (2325, 26)\n",
            "All Ones (sklearn): train=0.281, test=0.280\n",
            "All Ones (keras): train=0.281, test=0.280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loved-lodge"
      },
      "source": [
        "# baseline model for the planet dataset\n",
        "import sys\n",
        "from numpy import load\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.optimizers import SGD"
      ],
      "id": "loved-lodge",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "secondary-place"
      },
      "source": [
        "# define cnn model\n",
        "def define_model(in_shape=(128, 128, 3), out_shape=26):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=in_shape))\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "  model.add(Dense(out_shape, activation='sigmoid'))\n",
        "  # compile model\n",
        "  opt = SGD(lr=0.01, momentum=0.9)\n",
        "  model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=[fbeta])\n",
        "  return model"
      ],
      "id": "secondary-place",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arctic-graphic"
      },
      "source": [
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history, name=\"\"):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Cross Entropy Loss')\n",
        "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Fbeta')\n",
        "\tpyplot.plot(history.history['fbeta'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_fbeta'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "\tfilename = sys.argv[0].split('/')[-1]\n",
        "\tpyplot.savefig(filename + '_' + name + '_plot.png')\n",
        "\tpyplot.close()"
      ],
      "id": "arctic-graphic",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "identified-pendant"
      },
      "source": [
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "  # load dataset\n",
        "  trainX, trainY, testX, testY = load_dataset()\n",
        "  # create data generator\n",
        "  datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
        "                              featurewise_center=True,\n",
        "                              featurewise_std_normalization=True,\n",
        "                              rotation_range=20,\n",
        "                              width_shift_range=0.2,\n",
        "                              height_shift_range=0.2,\n",
        "                              horizontal_flip=True,\n",
        "                              validation_split=0.2)\n",
        "  # prepare iterators\n",
        "  train_it = datagen.flow(trainX, trainY, batch_size=128)\n",
        "  test_it = datagen.flow(testX, testY, batch_size=128)\n",
        "  # define model\n",
        "  model = define_model()\n",
        "  # fit model\n",
        "  history = model.fit(train_it, steps_per_epoch=len(train_it),\n",
        "    validation_data=test_it, validation_steps=len(test_it), epochs=20, batch_size=64)\n",
        "  # evaluate model\n",
        "  loss, fbeta = model.evaluate(test_it, steps=len(test_it), verbose=0)\n",
        "  print('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n",
        "  # learning curves\n",
        "  summarize_diagnostics(history)\n",
        "  return model"
      ],
      "id": "identified-pendant",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sapphire-illness",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "7685806c-3533-48ec-94c0-36677588804d"
      },
      "source": [
        "model = run_test_harness()"
      ],
      "id": "sapphire-illness",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-714a99ac28d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_test_harness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-4f263c4e6965>\u001b[0m in \u001b[0;36mrun_test_harness\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_test_harness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m# load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;31m# create data generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
            "\u001b[0;32m<ipython-input-3-f43935aa6764>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive/MyDrive/planet_data.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m# separate into train and test datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 return format.read_array(bytes,\n\u001b[1;32m    254\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                                          pickle_kwargs=self.pickle_kwargs)\n\u001b[0m\u001b[1;32m    256\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m                     \u001b[0mread_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_read_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                     \u001b[0mread_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_count\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m                     array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[1;32m    765\u001b[0m                                                              count=read_count)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    890\u001b[0m         \u001b[0;31m# done about that.  note that regular files can't be non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    996\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconsumed_tail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m_read2\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    752\u001b[0m                         \"Close the writing handle before trying to read.\")\n\u001b[1;32m    753\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Cqf7wbVXYRh"
      },
      "source": [
        "model.save(\"./score_3135.h5\")"
      ],
      "id": "3Cqf7wbVXYRh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MklNUNnYewPk",
        "outputId": "3651150c-db98-4e0c-957d-6158f0b9cbef"
      },
      "source": [
        "!zip -r score_3135.zip score_3135"
      ],
      "id": "MklNUNnYewPk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: score_3135/ (stored 0%)\n",
            "  adding: score_3135/assets/ (stored 0%)\n",
            "  adding: score_3135/keras_metadata.pb (deflated 93%)\n",
            "  adding: score_3135/saved_model.pb (deflated 90%)\n",
            "  adding: score_3135/variables/ (stored 0%)\n",
            "  adding: score_3135/variables/variables.data-00000-of-00001 (deflated 25%)\n",
            "  adding: score_3135/variables/variables.index (deflated 70%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LaiXPHTi5vz"
      },
      "source": [
        "# define cnn model\n",
        "def define_model(in_shape=(128, 128, 3), out_shape=26):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=in_shape))\n",
        "  model.add(Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "  model.add(Dense(out_shape, activation='sigmoid'))\n",
        "  # compile model\n",
        "  opt = SGD(learning_rate=0.03, momentum=0.9)\n",
        "  model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n",
        "  return model"
      ],
      "id": "5LaiXPHTi5vz",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-e40llBfQ7J"
      },
      "source": [
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "  # load dataset\n",
        "  trainX, trainY, testX, testY = load_dataset()\n",
        "  # create data generator\n",
        "  datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
        "                              featurewise_center=True,\n",
        "                              featurewise_std_normalization=True,\n",
        "                              rotation_range=20,\n",
        "                              width_shift_range=0.2,\n",
        "                              height_shift_range=0.2,\n",
        "                              horizontal_flip=True,\n",
        "                              validation_split=0.2)\n",
        "  # prepare iterators\n",
        "  train_it = datagen.flow(trainX, trainY, batch_size=128)\n",
        "  test_it = datagen.flow(testX, testY, batch_size=128)\n",
        "  # define model\n",
        "  model = define_model()\n",
        "  # fit model\n",
        "  history = model.fit(train_it, steps_per_epoch=len(train_it),\n",
        "    validation_data=test_it, validation_steps=len(test_it), epochs=20,\n",
        "    batch_size=64)\n",
        "  # evaluate model\n",
        "  loss, fbeta = model.evaluate(test_it, steps=len(test_it), verbose=0)\n",
        "  print('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n",
        "  # learning curves\n",
        "  summarize_diagnostics(history, str(int(fbeta*1000)))\n",
        "  return model"
      ],
      "id": "D-e40llBfQ7J",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKhct8hBf4cJ",
        "outputId": "9197d2fa-0885-47f6-e9e3-53a195c64c47"
      },
      "source": [
        "model = run_test_harness()"
      ],
      "id": "hKhct8hBf4cJ",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5424, 128, 128, 3) (5424, 26) (2325, 128, 128, 3) (2325, 26)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "43/43 [==============================] - 26s 558ms/step - loss: 0.4299 - fbeta: 0.2477 - val_loss: 0.2849 - val_fbeta: 0.3782\n",
            "Epoch 2/20\n",
            "43/43 [==============================] - 23s 530ms/step - loss: 0.2182 - fbeta: 0.3973 - val_loss: 0.2677 - val_fbeta: 0.3891\n",
            "Epoch 3/20\n",
            "43/43 [==============================] - 23s 534ms/step - loss: 0.2189 - fbeta: 0.3904 - val_loss: 0.2436 - val_fbeta: 0.3799\n",
            "Epoch 4/20\n",
            "43/43 [==============================] - 23s 532ms/step - loss: 0.2218 - fbeta: 0.3889 - val_loss: 0.2608 - val_fbeta: 0.3809\n",
            "Epoch 5/20\n",
            "43/43 [==============================] - 23s 535ms/step - loss: 0.2179 - fbeta: 0.3863 - val_loss: 0.2637 - val_fbeta: 0.3857\n",
            "Epoch 6/20\n",
            "43/43 [==============================] - 23s 528ms/step - loss: 0.2163 - fbeta: 0.4066 - val_loss: 0.2566 - val_fbeta: 0.3754\n",
            "Epoch 7/20\n",
            "43/43 [==============================] - 23s 532ms/step - loss: 0.2173 - fbeta: 0.3929 - val_loss: 0.2622 - val_fbeta: 0.3835\n",
            "Epoch 8/20\n",
            "43/43 [==============================] - 23s 533ms/step - loss: 0.2183 - fbeta: 0.3938 - val_loss: 0.2570 - val_fbeta: 0.3881\n",
            "Epoch 9/20\n",
            "43/43 [==============================] - 23s 531ms/step - loss: 0.2158 - fbeta: 0.3981 - val_loss: 0.2638 - val_fbeta: 0.3860\n",
            "Epoch 10/20\n",
            "43/43 [==============================] - 23s 533ms/step - loss: 0.2152 - fbeta: 0.4006 - val_loss: 0.2536 - val_fbeta: 0.2147\n",
            "Epoch 11/20\n",
            "43/43 [==============================] - 23s 532ms/step - loss: 0.2177 - fbeta: 0.3886 - val_loss: 0.2521 - val_fbeta: 0.3883\n",
            "Epoch 12/20\n",
            "43/43 [==============================] - 23s 531ms/step - loss: 0.2156 - fbeta: 0.3946 - val_loss: 0.2545 - val_fbeta: 0.3841\n",
            "Epoch 13/20\n",
            "43/43 [==============================] - 23s 533ms/step - loss: 0.2152 - fbeta: 0.3972 - val_loss: 0.2576 - val_fbeta: 0.0230\n",
            "Epoch 14/20\n",
            "43/43 [==============================] - 23s 535ms/step - loss: 0.2183 - fbeta: 0.3898 - val_loss: 0.2596 - val_fbeta: 0.0211\n",
            "Epoch 15/20\n",
            "43/43 [==============================] - 23s 530ms/step - loss: 0.2185 - fbeta: 0.3819 - val_loss: 0.2574 - val_fbeta: 0.1931\n",
            "Epoch 16/20\n",
            "43/43 [==============================] - 23s 529ms/step - loss: 0.2164 - fbeta: 0.3728 - val_loss: 0.2453 - val_fbeta: 0.1330\n",
            "Epoch 17/20\n",
            "43/43 [==============================] - 23s 532ms/step - loss: 0.2168 - fbeta: 0.3784 - val_loss: 0.2554 - val_fbeta: 0.0443\n",
            "Epoch 18/20\n",
            "43/43 [==============================] - 23s 533ms/step - loss: 0.2149 - fbeta: 0.3714 - val_loss: 0.2410 - val_fbeta: 0.0546\n",
            "Epoch 19/20\n",
            "43/43 [==============================] - 23s 534ms/step - loss: 0.2138 - fbeta: 0.3707 - val_loss: 0.2440 - val_fbeta: 0.1120\n",
            "Epoch 20/20\n",
            "43/43 [==============================] - 23s 531ms/step - loss: 0.2151 - fbeta: 0.3620 - val_loss: 0.2590 - val_fbeta: 0.0087\n",
            "> loss=0.259, fbeta=0.010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaNuuCmGgbLp",
        "outputId": "3c9c8cf2-f6e8-460e-9626-cfa05e48b689"
      },
      "source": [
        "model.save(\"./score_0087.h5\")\n",
        "model.save(\"./score_0087\")"
      ],
      "id": "UaNuuCmGgbLp",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./score_0087/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT7Ejw6bnN4K",
        "outputId": "97a86f5f-615c-4912-d03c-7a74742891c4"
      },
      "source": [
        "!zip -r score_0087.zip score_0087"
      ],
      "id": "CT7Ejw6bnN4K",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: score_0087/ (stored 0%)\n",
            "  adding: score_0087/assets/ (stored 0%)\n",
            "  adding: score_0087/keras_metadata.pb (deflated 94%)\n",
            "  adding: score_0087/saved_model.pb (deflated 90%)\n",
            "  adding: score_0087/variables/ (stored 0%)\n",
            "  adding: score_0087/variables/variables.data-00000-of-00001 (deflated 20%)\n",
            "  adding: score_0087/variables/variables.index (deflated 71%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWQ9wWvI2Z2r",
        "outputId": "148b2e1c-e009-4607-bed9-f865255eca98"
      },
      "source": [
        "trainX.shape"
      ],
      "id": "iWQ9wWvI2Z2r",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5424, 128, 128, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMi-QX9t6gz6"
      },
      "source": [
        ""
      ],
      "id": "UMi-QX9t6gz6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9MOVmxvnR7C"
      },
      "source": [
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "  # load dataset\n",
        "  trainX, trainY, testX, testY = load_dataset()\n",
        "  # create data generator\n",
        "  datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
        "                              featurewise_center=True,\n",
        "                              featurewise_std_normalization=True,\n",
        "                              rotation_range=20,\n",
        "                              width_shift_range=0.2,\n",
        "                              height_shift_range=0.2,\n",
        "                              horizontal_flip=True)\n",
        "  # prepare iterators\n",
        "  train_it = datagen.flow(trainX, trainY, batch_size=32)\n",
        "  test_it = datagen.flow(testX, testY, batch_size=32)\n",
        "  # define model\n",
        "  model = define_model()\n",
        "  # fit model\n",
        "  history = model.fit(train_it, steps_per_epoch=165,\n",
        "    validation_data=test_it, epochs=80,\n",
        "    batch_size=32,shuffle=True)\n",
        "  # evaluate model\n",
        "  loss, fbeta = model.evaluate(test_it, steps=len(test_it), verbose=0)\n",
        "  print('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n",
        "  # learning curves\n",
        "  summarize_diagnostics(history, str(int(fbeta*1000)))\n",
        "  return model"
      ],
      "id": "S9MOVmxvnR7C",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Sa5uI8C2NpE",
        "outputId": "81fe8ff0-3a09-418a-8bdf-6b7961908323"
      },
      "source": [
        "model = run_test_harness()"
      ],
      "id": "8Sa5uI8C2NpE",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5424, 128, 128, 3) (5424, 26) (2325, 128, 128, 3) (2325, 26)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/80\n",
            "165/165 [==============================] - 65s 144ms/step - loss: 0.2861 - fbeta: 0.3497 - val_loss: 0.2408 - val_fbeta: 0.3201\n",
            "Epoch 2/80\n",
            "165/165 [==============================] - 23s 138ms/step - loss: 0.2178 - fbeta: 0.3346 - val_loss: 0.2354 - val_fbeta: 0.3824\n",
            "Epoch 3/80\n",
            "165/165 [==============================] - 23s 139ms/step - loss: 0.2176 - fbeta: 0.3021 - val_loss: 0.2325 - val_fbeta: 0.3865\n",
            "Epoch 4/80\n",
            "165/165 [==============================] - 23s 139ms/step - loss: 0.2204 - fbeta: 0.3738 - val_loss: 0.2311 - val_fbeta: 0.3866\n",
            "Epoch 5/80\n",
            "165/165 [==============================] - 23s 138ms/step - loss: 0.2202 - fbeta: 0.3526 - val_loss: 0.2276 - val_fbeta: 0.3862\n",
            "Epoch 6/80\n",
            "165/165 [==============================] - 23s 139ms/step - loss: 0.2177 - fbeta: 0.3885 - val_loss: 0.2308 - val_fbeta: 0.2511\n",
            "Epoch 7/80\n",
            "165/165 [==============================] - 23s 139ms/step - loss: 0.2153 - fbeta: 0.3593 - val_loss: 0.2230 - val_fbeta: 0.3478\n",
            "Epoch 8/80\n",
            "165/165 [==============================] - 23s 138ms/step - loss: 0.2190 - fbeta: 0.3284 - val_loss: 0.2222 - val_fbeta: 0.3202\n",
            "Epoch 9/80\n",
            "165/165 [==============================] - 23s 139ms/step - loss: 0.2174 - fbeta: 0.3237 - val_loss: 0.2215 - val_fbeta: 0.2629\n",
            "Epoch 10/80\n",
            "165/165 [==============================] - 23s 138ms/step - loss: 0.2220 - fbeta: 0.3096 - val_loss: 0.2233 - val_fbeta: 0.2436\n",
            "Epoch 11/80\n",
            "165/165 [==============================] - 23s 139ms/step - loss: 0.2154 - fbeta: 0.3245 - val_loss: 0.2264 - val_fbeta: 0.1208\n",
            "Epoch 12/80\n",
            "165/165 [==============================] - 23s 138ms/step - loss: 0.2113 - fbeta: 0.3275 - val_loss: 0.2342 - val_fbeta: 0.0738\n",
            "Epoch 13/80\n",
            "165/165 [==============================] - 23s 138ms/step - loss: 0.2136 - fbeta: 0.3217 - val_loss: 0.2230 - val_fbeta: 0.1277\n",
            "Epoch 14/80\n",
            "165/165 [==============================] - 26s 161ms/step - loss: 0.2160 - fbeta: 0.2892 - val_loss: 0.2194 - val_fbeta: 0.3276\n",
            "Epoch 15/80\n",
            "165/165 [==============================] - 23s 137ms/step - loss: 0.2149 - fbeta: 0.3426 - val_loss: 0.2217 - val_fbeta: 0.1554\n",
            "Epoch 16/80\n",
            "165/165 [==============================] - 23s 138ms/step - loss: 0.2152 - fbeta: 0.3034 - val_loss: 0.2233 - val_fbeta: 0.2057\n",
            "Epoch 17/80\n",
            "165/165 [==============================] - 23s 139ms/step - loss: 0.2150 - fbeta: 0.3278 - val_loss: 0.2196 - val_fbeta: 0.3023\n",
            "Epoch 18/80\n",
            "165/165 [==============================] - 23s 139ms/step - loss: 0.2171 - fbeta: 0.3056 - val_loss: 0.2231 - val_fbeta: 0.1366\n",
            "Epoch 19/80\n",
            "165/165 [==============================] - 23s 140ms/step - loss: 0.2159 - fbeta: 0.3410 - val_loss: 0.2191 - val_fbeta: 0.3158\n",
            "Epoch 20/80\n",
            "165/165 [==============================] - 23s 139ms/step - loss: 0.2144 - fbeta: 0.3218 - val_loss: 0.2229 - val_fbeta: 0.3134\n",
            "Epoch 21/80\n",
            "165/165 [==============================] - 23s 140ms/step - loss: 0.2157 - fbeta: 0.3328 - val_loss: 0.2231 - val_fbeta: 0.1387\n",
            "Epoch 22/80\n",
            "165/165 [==============================] - 23s 139ms/step - loss: 0.2125 - fbeta: 0.3145 - val_loss: 0.2254 - val_fbeta: 0.2626\n",
            "Epoch 23/80\n",
            "165/165 [==============================] - 23s 140ms/step - loss: 0.2137 - fbeta: 0.3267 - val_loss: 0.2201 - val_fbeta: 0.2869\n",
            "Epoch 24/80\n",
            "165/165 [==============================] - 23s 140ms/step - loss: 0.2136 - fbeta: 0.3222 - val_loss: 0.2187 - val_fbeta: 0.3668\n",
            "Epoch 25/80\n",
            "165/165 [==============================] - 23s 139ms/step - loss: 0.2138 - fbeta: 0.3335 - val_loss: 0.2204 - val_fbeta: 0.2757\n",
            "Epoch 26/80\n",
            "165/165 [==============================] - 23s 140ms/step - loss: 0.2147 - fbeta: 0.3172 - val_loss: 0.2243 - val_fbeta: 0.2227\n",
            "Epoch 27/80\n",
            "165/165 [==============================] - 23s 140ms/step - loss: 0.2139 - fbeta: 0.3149 - val_loss: 0.2228 - val_fbeta: 0.2785\n",
            "Epoch 28/80\n",
            "165/165 [==============================] - 23s 140ms/step - loss: 0.2139 - fbeta: 0.3319 - val_loss: 0.2213 - val_fbeta: 0.2713\n",
            "Epoch 29/80\n",
            "165/165 [==============================] - 23s 140ms/step - loss: 0.2160 - fbeta: 0.3364 - val_loss: 0.2179 - val_fbeta: 0.3109\n",
            "Epoch 30/80\n",
            "165/165 [==============================] - 23s 140ms/step - loss: 0.2119 - fbeta: 0.3354 - val_loss: 0.2230 - val_fbeta: 0.2143\n",
            "Epoch 31/80\n",
            "165/165 [==============================] - 23s 140ms/step - loss: 0.2162 - fbeta: 0.3270 - val_loss: 0.2195 - val_fbeta: 0.3317\n",
            "Epoch 32/80\n",
            "165/165 [==============================] - 23s 139ms/step - loss: 0.2152 - fbeta: 0.3342 - val_loss: 0.2258 - val_fbeta: 0.2830\n",
            "Epoch 33/80\n",
            "165/165 [==============================] - 23s 140ms/step - loss: 0.2154 - fbeta: 0.3451 - val_loss: 0.2172 - val_fbeta: 0.2360\n",
            "Epoch 34/80\n",
            "165/165 [==============================] - 23s 140ms/step - loss: 0.2126 - fbeta: 0.3193 - val_loss: 0.2244 - val_fbeta: 0.3229\n",
            "Epoch 35/80\n",
            "165/165 [==============================] - 23s 140ms/step - loss: 0.2177 - fbeta: 0.3255 - val_loss: 0.2224 - val_fbeta: 0.1936\n",
            "Epoch 36/80\n",
            "165/165 [==============================] - 23s 140ms/step - loss: 0.2160 - fbeta: 0.3297 - val_loss: 0.2204 - val_fbeta: 0.2105\n",
            "Epoch 37/80\n",
            "165/165 [==============================] - 23s 140ms/step - loss: 0.2140 - fbeta: 0.3222 - val_loss: 0.2227 - val_fbeta: 0.2122\n",
            "Epoch 38/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2135 - fbeta: 0.3385 - val_loss: 0.2169 - val_fbeta: 0.3087\n",
            "Epoch 39/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2173 - fbeta: 0.3198 - val_loss: 0.2216 - val_fbeta: 0.1961\n",
            "Epoch 40/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2140 - fbeta: 0.3376 - val_loss: 0.2171 - val_fbeta: 0.3193\n",
            "Epoch 41/80\n",
            "165/165 [==============================] - 23s 139ms/step - loss: 0.2108 - fbeta: 0.3502 - val_loss: 0.2186 - val_fbeta: 0.2492\n",
            "Epoch 42/80\n",
            "165/165 [==============================] - 23s 139ms/step - loss: 0.2147 - fbeta: 0.3287 - val_loss: 0.2160 - val_fbeta: 0.3192\n",
            "Epoch 43/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2136 - fbeta: 0.3436 - val_loss: 0.2252 - val_fbeta: 0.1167\n",
            "Epoch 44/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2165 - fbeta: 0.3240 - val_loss: 0.2234 - val_fbeta: 0.1894\n",
            "Epoch 45/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2131 - fbeta: 0.3253 - val_loss: 0.2197 - val_fbeta: 0.2980\n",
            "Epoch 46/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2136 - fbeta: 0.3560 - val_loss: 0.2188 - val_fbeta: 0.2893\n",
            "Epoch 47/80\n",
            "165/165 [==============================] - 23s 140ms/step - loss: 0.2157 - fbeta: 0.3314 - val_loss: 0.2184 - val_fbeta: 0.3047\n",
            "Epoch 48/80\n",
            "165/165 [==============================] - 23s 140ms/step - loss: 0.2162 - fbeta: 0.3334 - val_loss: 0.2251 - val_fbeta: 0.1778\n",
            "Epoch 49/80\n",
            "165/165 [==============================] - 23s 140ms/step - loss: 0.2150 - fbeta: 0.3401 - val_loss: 0.2162 - val_fbeta: 0.2893\n",
            "Epoch 50/80\n",
            "165/165 [==============================] - 23s 140ms/step - loss: 0.2150 - fbeta: 0.3334 - val_loss: 0.2220 - val_fbeta: 0.2060\n",
            "Epoch 51/80\n",
            "165/165 [==============================] - 23s 142ms/step - loss: 0.2139 - fbeta: 0.3316 - val_loss: 0.2184 - val_fbeta: 0.2460\n",
            "Epoch 52/80\n",
            "165/165 [==============================] - 23s 142ms/step - loss: 0.2130 - fbeta: 0.3309 - val_loss: 0.2192 - val_fbeta: 0.3126\n",
            "Epoch 53/80\n",
            "165/165 [==============================] - 23s 143ms/step - loss: 0.2113 - fbeta: 0.3519 - val_loss: 0.2165 - val_fbeta: 0.3234\n",
            "Epoch 54/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2132 - fbeta: 0.3420 - val_loss: 0.2179 - val_fbeta: 0.2673\n",
            "Epoch 55/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2159 - fbeta: 0.3450 - val_loss: 0.2174 - val_fbeta: 0.2895\n",
            "Epoch 56/80\n",
            "165/165 [==============================] - 23s 143ms/step - loss: 0.2137 - fbeta: 0.3482 - val_loss: 0.2180 - val_fbeta: 0.2961\n",
            "Epoch 57/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2134 - fbeta: 0.3384 - val_loss: 0.2200 - val_fbeta: 0.2587\n",
            "Epoch 58/80\n",
            "165/165 [==============================] - 23s 142ms/step - loss: 0.2093 - fbeta: 0.3600 - val_loss: 0.2161 - val_fbeta: 0.3124\n",
            "Epoch 59/80\n",
            "165/165 [==============================] - 23s 142ms/step - loss: 0.2117 - fbeta: 0.3381 - val_loss: 0.2174 - val_fbeta: 0.2953\n",
            "Epoch 60/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2162 - fbeta: 0.3391 - val_loss: 0.2157 - val_fbeta: 0.3090\n",
            "Epoch 61/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2150 - fbeta: 0.3288 - val_loss: 0.2225 - val_fbeta: 0.1460\n",
            "Epoch 62/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2111 - fbeta: 0.3241 - val_loss: 0.2173 - val_fbeta: 0.3026\n",
            "Epoch 63/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2110 - fbeta: 0.3396 - val_loss: 0.2177 - val_fbeta: 0.2513\n",
            "Epoch 64/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2089 - fbeta: 0.3495 - val_loss: 0.2233 - val_fbeta: 0.2582\n",
            "Epoch 65/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2141 - fbeta: 0.3415 - val_loss: 0.2169 - val_fbeta: 0.3156\n",
            "Epoch 66/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2108 - fbeta: 0.3544 - val_loss: 0.2188 - val_fbeta: 0.3006\n",
            "Epoch 67/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2077 - fbeta: 0.3569 - val_loss: 0.2168 - val_fbeta: 0.2880\n",
            "Epoch 68/80\n",
            "165/165 [==============================] - 23s 140ms/step - loss: 0.2117 - fbeta: 0.3307 - val_loss: 0.2203 - val_fbeta: 0.2837\n",
            "Epoch 69/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2134 - fbeta: 0.3360 - val_loss: 0.2225 - val_fbeta: 0.2299\n",
            "Epoch 70/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2145 - fbeta: 0.3401 - val_loss: 0.2163 - val_fbeta: 0.3119\n",
            "Epoch 71/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2132 - fbeta: 0.3320 - val_loss: 0.2164 - val_fbeta: 0.3204\n",
            "Epoch 72/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2091 - fbeta: 0.3458 - val_loss: 0.2176 - val_fbeta: 0.3191\n",
            "Epoch 73/80\n",
            "165/165 [==============================] - 23s 142ms/step - loss: 0.2089 - fbeta: 0.3640 - val_loss: 0.2168 - val_fbeta: 0.3047\n",
            "Epoch 74/80\n",
            "165/165 [==============================] - 23s 143ms/step - loss: 0.2143 - fbeta: 0.3419 - val_loss: 0.2176 - val_fbeta: 0.2975\n",
            "Epoch 75/80\n",
            "165/165 [==============================] - 23s 142ms/step - loss: 0.2120 - fbeta: 0.3433 - val_loss: 0.2247 - val_fbeta: 0.2659\n",
            "Epoch 76/80\n",
            "165/165 [==============================] - 23s 142ms/step - loss: 0.2076 - fbeta: 0.3512 - val_loss: 0.2171 - val_fbeta: 0.3310\n",
            "Epoch 77/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2133 - fbeta: 0.3535 - val_loss: 0.2153 - val_fbeta: 0.3115\n",
            "Epoch 78/80\n",
            "165/165 [==============================] - 23s 142ms/step - loss: 0.2110 - fbeta: 0.3408 - val_loss: 0.2150 - val_fbeta: 0.2964\n",
            "Epoch 79/80\n",
            "165/165 [==============================] - 23s 142ms/step - loss: 0.2085 - fbeta: 0.3591 - val_loss: 0.2147 - val_fbeta: 0.3180\n",
            "Epoch 80/80\n",
            "165/165 [==============================] - 23s 141ms/step - loss: 0.2117 - fbeta: 0.3436 - val_loss: 0.2147 - val_fbeta: 0.3249\n",
            "> loss=0.215, fbeta=0.325\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCnD9aLI2PLz",
        "outputId": "02a42650-f925-4a37-9704-d074aa3377d0"
      },
      "source": [
        "model.save(\"./score_3249.h5\")\n",
        "model.save(\"./score_3249\")"
      ],
      "id": "uCnD9aLI2PLz",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./score_3249/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIiOn6SyIrbV",
        "outputId": "f65ceba2-cb63-4145-b056-0d43959e589b"
      },
      "source": [
        "!zip -r score_3249.zip score_3249"
      ],
      "id": "yIiOn6SyIrbV",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: score_3249/ (stored 0%)\n",
            "  adding: score_3249/assets/ (stored 0%)\n",
            "  adding: score_3249/keras_metadata.pb (deflated 94%)\n",
            "  adding: score_3249/saved_model.pb (deflated 89%)\n",
            "  adding: score_3249/variables/ (stored 0%)\n",
            "  adding: score_3249/variables/variables.data-00000-of-00001 (deflated 18%)\n",
            "  adding: score_3249/variables/variables.index (deflated 71%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGgRoy4lIttk"
      },
      "source": [
        ""
      ],
      "id": "xGgRoy4lIttk",
      "execution_count": null,
      "outputs": []
    }
  ]
}